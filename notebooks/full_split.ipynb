{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2928583-5503-49f3-90c4-1d86bbb32309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# standard python\n",
    "import numpy as np\n",
    "import scipy\n",
    "#import pathlib\n",
    "\n",
    "import os\n",
    "# plotting, especially for jupyter notebooks\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True # breaks for some endpoint labels\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() # needed for tf version 1 or it stages operations but does not do them\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "# local routines\n",
    "#from chemdataprep import load_PDBs,load_countsfromPDB,load_diametersfromPDB,find_chemnames\n",
    "#from toxmathandler import load_tscores\n",
    "\n",
    "#checkpoint_path = \"/home2/ajgreen4/Read-Across_w_GAN/Models/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__,\". Executing eagerly?\",tf.executing_eagerly())\n",
    "print(\"Number of GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964acb3-d6e8-462c-a11d-88300b4f25fb",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e408f-3a74-4d33-a2fd-eb7e000e7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data, val and test data\n",
    "file_path = 'train.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    train_df = pickle.load(file)\n",
    "#print(train_df)\n",
    "# good fit = lower negative number\n",
    "# more negative docking score = better fit \n",
    "\n",
    "\n",
    "# Load validation data\n",
    "file_path = 'validation_small_enantiomers_stable_full_screen_docking_MOL_margin3_49878_10368_5184.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    val_df = pickle.load(file)\n",
    "#print(val_df)\n",
    "\n",
    "\n",
    "# Load test data\n",
    "file_path = 'test_small_enantiomers_stable_full_screen_docking_MOL_margin3_50571_10368_5184.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    test_df = pickle.load(file)\n",
    "#print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0296a-4393-4560-af1e-9aa499cef9a1",
   "metadata": {},
   "source": [
    "# Get atoms in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5017bb-0e5e-4273-a2de-8017a60304c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86459db-cd7a-4246-81e4-8aadaffd2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def atomic_number_to_symbol(atomic_number):\n",
    "    periodic_table = {\n",
    "        1: 'H', 6: 'C', 7: 'N', 8: 'O', 9: 'F',\n",
    "        15: 'P', 16: 'S', 17: 'Cl', 35: 'Br', 53: 'I'\n",
    "    }\n",
    "    return periodic_table.get(atomic_number, 'Unknown')\n",
    "\n",
    "# Define dataset paths\n",
    "datasets = {\n",
    "    'train': 'train.pkl',\n",
    "    'val': 'validation_small_enantiomers_stable_full_screen_docking_MOL_margin3_49878_10368_5184.pkl',\n",
    "    'test': 'test_small_enantiomers_stable_full_screen_docking_MOL_margin3_50571_10368_5184.pkl'\n",
    "}\n",
    "\n",
    "mollists = {}\n",
    "\n",
    "for dataset_name, file_path in datasets.items():\n",
    "    with open(file_path, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    \n",
    "    mollist = []\n",
    "    for mol in df['rdkit_mol_cistrans_stereo']:\n",
    "        molecule = []\n",
    "        if mol is not None and isinstance(mol, Chem.Mol):\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                conf = mol.GetConformer(0)\n",
    "                for atom in mol.GetAtoms():\n",
    "                    atomic_number = atom.GetAtomicNum()\n",
    "                    atom_type = atomic_number_to_symbol(atomic_number)\n",
    "                    pos = conf.GetAtomPosition(atom.GetIdx())\n",
    "                    coordinates = np.array([pos.x, pos.y, pos.z])\n",
    "                    molecule.append((atom_type, coordinates))\n",
    "        mollist.append(molecule)\n",
    "    \n",
    "    mollists[dataset_name] = mollist\n",
    "    print(f\"{dataset_name.capitalize()} Sample (first molecule):\")\n",
    "    print(mollist[0] if mollist else \"No valid molecules found.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9ae50-94db-4151-893a-8d6ff9d8d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mollist = mollists['train']\n",
    "val_mollist = mollists['val']\n",
    "test_mollist = mollists['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2653344-bc7d-4941-a624-52e6169a2f17",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec1a24-2176-4694-acbe-96f24ff7dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = train_mollist    \n",
    "y_train = train_df['top_score']\n",
    "\n",
    "X_val = val_mollist             \n",
    "y_val = val_df['top_score']    \n",
    "\n",
    "X_test = test_mollist          \n",
    "y_test = test_df['top_score']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3179fd-f45d-4d7e-a615-8b7b0fc9325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for availabel atoms\n",
    "def speciesmap(atom_type):\n",
    "    atom_to_number = {\n",
    "        'H': 1,   # Hydrogen\n",
    "        'C': 6,   # Carbon\n",
    "        'N': 7,   # Nitrogen\n",
    "        'O': 8,   # Oxygen\n",
    "        'F': 9,   # Fluorine\n",
    "        'P': 15,  # Phosphorus\n",
    "        'S': 16,  # Sulfur\n",
    "        'Cl': 17, # Chlorine\n",
    "        'Br': 35, # Bromine\n",
    "        'I': 53   # Iodine\n",
    "    }\n",
    "    return np.array([atom_to_number.get(atom_type, 0)])  # Returns 0 if atom type is not recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0774ca-2fdf-4cdb-b065-75e5c0b74e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Get weight and Views for training, validation and test dataset\n",
    "'''\n",
    "from qm7_weightedviews_train import load_qm7_data\n",
    "from qm7_weightedviews_val import load_qm7_data\n",
    "from qm7_weightedviews_test import load_qm7_data\n",
    "\n",
    "\n",
    "ws_train, vs_train, Natoms_train, Nviews_train = load_qm7_data(train_mollist, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, verbose=0)\n",
    "chiral_train = [ws_train, vs_train]\n",
    "\n",
    "# Featurize validation data\n",
    "ws_val, vs_val, Natoms_val, Nviews_val = load_qm7_data(val_mollist, speciesmap, setNatoms=29, setNviews=29, carbonbased=False, verbose=0)\n",
    "chiral_val = [ws_val, vs_val]\n",
    "\n",
    "# Featurize test data\n",
    "ws_test, vs_test, Natoms_test, Nviews_test = load_qm7_data(test_mollist, speciesmap, setNatoms=29, setNviews=29, carbonbased=False, verbose=0)\n",
    "chiral_test = [ws_test, vs_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ef89f-9632-411c-a829-82f3e9210664",
   "metadata": {},
   "source": [
    "# Alternative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2402b-80d6-40e1-9692-b237f6687c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights and views\n",
    "ws_train = np.load('ws_train.npy')\n",
    "vs_train = np.load('vs_train.npy')\n",
    "ws_val = np.load('ws_val.npy')\n",
    "vs_val = np.load('vs_val.npy')\n",
    "ws_test = np.load('ws_test.npy')\n",
    "vs_test = np.load('vs_test.npy')\n",
    "\n",
    "# Load dimensions\n",
    "Natoms_train = np.load('Natoms_train.npy')\n",
    "Nviews_train = np.load('Nviews_train.npy')\n",
    "Natoms_val = np.load('Natoms_val.npy')\n",
    "Nviews_val = np.load('Nviews_val.npy')\n",
    "Natoms_test = np.load('Natoms_test.npy')\n",
    "Nviews_test = np.load('Nviews_test.npy')\n",
    "\n",
    "chiral_train = [ws_train, vs_train]\n",
    "chiral_val = [ws_val, vs_val]\n",
    "chiral_test = [ws_test, vs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd9831-526b-43d2-a714-3925c669b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of weights and views\n",
    "\n",
    "print(ws_train.shape)\n",
    "print(vs_train.shape)\n",
    "print(ws_val.shape)\n",
    "print(vs_val.shape)\n",
    "print(ws_test.shape)\n",
    "print(vs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9238e5e-e4a2-493a-8228-ad5c0643d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelsG_train = y_train \n",
    "labelsG_val = y_val     \n",
    "labelsG_test = y_test   \n",
    "\n",
    "Ntoxicity = 3  \n",
    "\n",
    "ws_train, vs_train = ws_train, vs_train \n",
    "ws_val, vs_val = ws_val, vs_val         \n",
    "ws_test, vs_test = ws_test, vs_test     \n",
    "\n",
    "dataG_train = [ws_train, vs_train]\n",
    "dataG_val = [ws_val, vs_val]              \n",
    "dataG_test = [ws_test, vs_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1743aca-5c1b-43ae-b68f-76143235770a",
   "metadata": {},
   "source": [
    "# Neural Network Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20502f2-0a44-4c6a-9656-cf09aeab4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic dense NN\n",
    "def multiDense(Nin,Nout,Nhidden,widthhidden=None,kernel_regularizer=None):\n",
    "    \"\"\"Construct a basic NN with some dense layers.\n",
    "    \n",
    "    :parameter Nin: The number of inputs\n",
    "    :type Nin: int\n",
    "    :parameter Nout: The number of outputs\n",
    "    :type Nout: int\n",
    "    :parameter Nhidden: The number of hidden layers.\n",
    "    :type Nhidden: int\n",
    "    :parameter widthhidden: The width of each hidden layer.\n",
    "        If left at None, Nin + Nout will be used.\n",
    "    :parameter kernel_regularizer: the regularizer to use, such as regularizers.l2(0.001)\n",
    "    :type kernel_regularizer: tensorflow.keras.regularizers.xxx\n",
    "    :returns: The NN model\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    if widthhidden is None:\n",
    "        widthhidden = Nin + Nout\n",
    "    x = inputs = keras.Input(shape=(Nin,), name='multiDense_input')\n",
    "    if kernel_regularizer is not None:\n",
    "        print(\"Using regularization\")\n",
    "    for i in range(Nhidden):\n",
    "        x = layers.Dense(widthhidden, activation='relu', kernel_regularizer=kernel_regularizer,name='dense'+str(i))(x)\n",
    "#        x = layers.Dense(widthhidden, name='dense'+str(i))(x)\n",
    "#        x = tf.nn.leaky_relu(x, alpha=0.05)\n",
    "#    outputs = layers.Dense(Nout, activation='linear',name='multiDense_output')(x)\n",
    "    outputs = layers.Dense(Nout,name='multiDense_output')(x)\n",
    "    #outputs = tf.nn.leaky_relu(outputs, alpha=0.05)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)#, name='multiDense')\n",
    "if 1:\n",
    "    # manual check of multiDense\n",
    "    mmd = multiDense(116,3,4,256)\n",
    "    mmd.summary()\n",
    "    # used to do the weighted sum over views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e46eec-a9c4-40cd-ba56-972e39053329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelwrapper(Nparallel,basemodel,insteadmax=False):\n",
    "    \"\"\"Construct a model that applies a basemodel multiple times and take a weighted sum (or max) of the result.\n",
    "    \n",
    "    :parameter Nparallel: The number of times to apply in parallel\n",
    "    :type Nparallel: int\n",
    "    :parameter basemodel: a keras.Model inferred to have Nin inputs and Nout outputs.\n",
    "    :type basemodel: a keras.Model\n",
    "    :parameter insteadmax: If True, take the max of the results of the basemodel instead of the weighted sum.\n",
    "        For compatibility, the model is still constructed with weights as inputs, but it ignores them.\n",
    "    :type insteadmax: Boolean\n",
    "    :returns: model with inputs shape [(?,Nparallel),(?,Nin,Nparallel)] and outputs shape (?,Nout).\n",
    "        The first input is the scalar weights in the sum.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    Note: We could do a max over the parallel applications instead of or in addition to the weighted sum.\n",
    "    \n",
    "    \"\"\"\n",
    "    # infer shape of basemodel inputs and outputs\n",
    "    Nin =  basemodel.inputs[0].shape[1]\n",
    "    Nout =  basemodel.outputs[0].shape[1]\n",
    "    \n",
    "    # Apply basemodel Nparallel times in parallel\n",
    "    # create main input (?,Nparallel,Nin) \n",
    "    parallel_inputs = keras.Input(shape=(Nparallel,Nin), name='parallelwrapper_input0')\n",
    "    # apply base NN to each parallel slice; outputs (?,Nparallel,Nout)\n",
    "    if False:\n",
    "        # original version, stopped working at some tensorflow update\n",
    "        xb = basemodel(parallel_inputs) # worked in earlier tensorflow\n",
    "        #xb = tf.map_fn(basemodel,parallel_inputs) # another version that fails\n",
    "    else:\n",
    "        # newer version, works but makes summary and graphing cumbersome\n",
    "        # unstack in the Nparallel directio\n",
    "        parallel_inputsunstacked = tf.keras.ops.unstack(parallel_inputs, Nparallel, 1)\n",
    "        # apply base NN to each \n",
    "        xbunstacked = [basemodel(x) for x in parallel_inputsunstacked]\n",
    "        # re-stack\n",
    "        xb = tf.keras.ops.stack(xbunstacked,axis=1)\n",
    "    \n",
    "    # create input scalars for weighted sun (?,Nparallel)\n",
    "    weight_inputs = keras.Input(shape=(Nparallel,), name='parallelScalars')\n",
    "    if insteadmax:\n",
    "        # take max over the Nparallel direction to get (?,1,Nout)\n",
    "        out = layers.MaxPool1D(pool_size=Nparallel)(xb)\n",
    "        # reshape to (?,Nout)\n",
    "        out = layers.Reshape((Nout,))(out)\n",
    "    else:\n",
    "        # do a weighted sum over the Nparallel direction to get (?,Nout)\n",
    "        out = layers.Dot((-2,-1))([xb,weight_inputs])\n",
    "    \n",
    "    return keras.Model(inputs=[weight_inputs,parallel_inputs], outputs=out, name='parallelwrapper')\n",
    "if 1:\n",
    "    # manual check\n",
    "    mmd = multiDense(116,3,4,256)\n",
    "    mpw = parallelwrapper(29,mmd,insteadmax=0)\n",
    "    mpw.summary()\n",
    "    # make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe63a0-7922-4020-b991-882bd59066bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_generator(data, labels, baselayers, Nfeatures, endlayers, Nviews, base_regularizer=None, end_regularizer=None):\n",
    "    \"\"\"Initialize the generator neural net.\n",
    "    \n",
    "    :parameter Nviews: Number of views/parallel applications\n",
    "    :type Nviews: int\n",
    "    :returns: return generator and descrimina NN.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Option changing how results of each view are aggregated\n",
    "    insteadmax = False # Does weighted average; original design\n",
    "    #insteadmax = True # Does max instead of weighted average (for both G and D)\n",
    "\n",
    "    # G\n",
    "    # base NN\n",
    "    Gbase = multiDense(data[1].shape[2], Nfeatures, baselayers, kernel_regularizer=base_regularizer) \n",
    "    # parallel view wrapper\n",
    "    Gpw = parallelwrapper(Nviews, Gbase, insteadmax)  # Now using the passed Nviews parameter\n",
    "    # features to toxicity\n",
    "    #Gft = multiDense(Nfeatures, labels.shape[1], endlayers, kernel_regularizer=end_regularizer)\n",
    "    #we got an error when running original line since energies are one dimnsional array so we can interpret the single dimension as having only one dimension\n",
    "    Gft = multiDense(Nfeatures, 1, endlayers, kernel_regularizer=end_regularizer) \n",
    "    # string together\n",
    "    generator = keras.Model(inputs=Gpw.inputs, outputs=Gft(Gpw.outputs), name='generator')\n",
    "    # make trainable\n",
    "    generator.compile(optimizer='adam', loss='mse')\n",
    "    #generator.summary()\n",
    "    # previously did better with Nfeatures=Ntoxicity and no Gft\n",
    "\n",
    "    if 0:\n",
    "        # sanity checks that model is working\n",
    "        print(\"Sanity check:\")\n",
    "        ws, vs = data\n",
    "        gbv0call = Gbase(vs[:,0,:]).numpy()\n",
    "        gbv0predict = Gbase.predict(vs[:,0,:])\n",
    "        print(\"base: 0 ?==\", np.linalg.norm(gbv0call-gbv0predict))\n",
    "        gpwcall = Gpw([ws,vs]).numpy()\n",
    "        gpwpredict = G\n",
    "        pw.predict([ws,vs])\n",
    "        print(\"wrapper: 0 ?==\", np.linalg.norm(gpwcall-gpwpredict))\n",
    "        gencall = generator([ws,vs]).numpy()\n",
    "        genpredict = generator.predict([ws,vs])\n",
    "        print(\"whole: 0 ?==\", np.linalg.norm(gencall-genpredict))\n",
    "        \n",
    "    return generator\n",
    "\n",
    "# Set your fixed Nviews value\n",
    "Nviews = 29  # Your chosen fixed value\n",
    "\n",
    "baselayers = 2  # hidden layers before weighted sum\n",
    "base_reg = 0  # regularization for the base layers\n",
    "Nfeatures = 3  # number of outputs of weighted sum\n",
    "endlayers = 2   # hidden layers after weighted sum\n",
    "end_reg = 0.1  # regularization for the end layers\n",
    "\n",
    "if base_reg == 0:\n",
    "    base_regularizer = None\n",
    "else:\n",
    "    base_regularizer = regularizers.l2(base_reg)\n",
    "\n",
    "if end_reg == 0:\n",
    "    end_regularizer = None\n",
    "else:\n",
    "    end_regularizer = regularizers.l2(end_reg)\n",
    "\n",
    "print(\"(baselayers, base_reg, Nfeatures, endlayers, end_reg, Nviews) =\",\n",
    "      (baselayers, base_reg, Nfeatures, endlayers, end_reg, Nviews))\n",
    "\n",
    "# compile model with options - NOW INCLUDING Nviews\n",
    "generator = init_generator(dataG_train, labelsG_train, baselayers, Nfeatures, endlayers, Nviews,\n",
    "                           base_regularizer=base_regularizer, end_regularizer=end_regularizer)\n",
    "generator.compile(optimizer='adam', loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bce75e-6fe1-4f76-997f-c86ec3ab5a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_enantiomer_pairs(df):\n",
    "    \"\"\"\n",
    "    Extract enantiomer pairs from dataframe based on SMILES_nostereo\n",
    "    first create groups by same enantiomer where molecules with same enantiomer are in same group\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    smiles_groups = df.groupby('SMILES_nostereo')\n",
    "    \n",
    "    for smiles, group in smiles_groups:\n",
    "        if len(group) >= 2:\n",
    "            score_groups = group.groupby('top_score')\n",
    "            enantiomer_data = []\n",
    "            \n",
    "            for score, score_group in score_groups:\n",
    "                enantiomer_data.append({\n",
    "                    'conformers': score_group.index.tolist(),\n",
    "                    'top_score': score,\n",
    "                    'size': len(score_group)\n",
    "                })\n",
    "            \n",
    "            if len(enantiomer_data) == 2:\n",
    "                # randomly assign which is enantiomer1 and enantiomer2\n",
    "                if np.random.random() > 0.5:\n",
    "                    # Assign group 0 to enantiomer1, group 1 to enantiomer2\n",
    "                    pairs.append({\n",
    "                        'enantiomer1_conformers': enantiomer_data[0]['conformers'],\n",
    "                        'enantiomer2_conformers': enantiomer_data[1]['conformers'],\n",
    "                        'enantiomer1_score': enantiomer_data[0]['top_score'],\n",
    "                        'enantiomer2_score': enantiomer_data[1]['top_score'],\n",
    "                        'smiles_nostereo': smiles\n",
    "                    })\n",
    "                else:\n",
    "                    # Assign group 1 to enantiomer1, group 0 to enantiomer2 \n",
    "                    pairs.append({\n",
    "                        'enantiomer1_conformers': enantiomer_data[1]['conformers'],\n",
    "                        'enantiomer2_conformers': enantiomer_data[0]['conformers'],\n",
    "                        'enantiomer1_score': enantiomer_data[1]['top_score'],\n",
    "                        'enantiomer2_score': enantiomer_data[0]['top_score'],\n",
    "                        'smiles_nostereo': smiles\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(pairs)} enantiomer pairs\")\n",
    "    return pairs\n",
    "\n",
    "def create_enantiomer_batch(pairs, data, labels, batch_size=32):\n",
    "    \"\"\"\n",
    "    pick a conformer from one bucket and other from second bucket\n",
    "    \"\"\"\n",
    "    batch_ws = []\n",
    "    batch_vs = []\n",
    "    batch_scores = []\n",
    "    \n",
    "    n_pairs = len(pairs)\n",
    "    if n_pairs < batch_size // 2:\n",
    "        selected_pairs = np.random.choice(pairs, size=batch_size//2, replace=True)\n",
    "    else:\n",
    "        selected_pairs = np.random.choice(pairs, size=batch_size//2, replace=False) # with replacement \n",
    "    \n",
    "    for pair in selected_pairs:\n",
    "        conf1_idx = np.random.choice(pair['enantiomer1_conformers'])\n",
    "        conf2_idx = np.random.choice(pair['enantiomer2_conformers'])\n",
    "        \n",
    "        batch_ws.extend([data[0][conf1_idx], data[0][conf2_idx]])\n",
    "        batch_vs.extend([data[1][conf1_idx], data[1][conf2_idx]])\n",
    "        batch_scores.extend([labels[conf1_idx], labels[conf2_idx]])\n",
    "    \n",
    "    return np.array(batch_ws), np.array(batch_vs), np.array(batch_scores).reshape(-1, 1)\n",
    "\n",
    "def train_with_enantiomer_sampling(generator, dataG_train, labelsG_train, train_df, \n",
    "                                 dataG_val, labelsG_val, val_df, epochs=10, batch_size=32):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    enantiomer_pairs_train = get_enantiomer_pairs(train_df) #extract pairs from training data\n",
    "    \n",
    "    if not enantiomer_pairs_train:\n",
    "        raise ValueError(\"No enantiomer pairs found in training data!\")\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_weights = None\n",
    "    patience = 1\n",
    "    patience_counter = 0\n",
    "    \n",
    "    num_batches_per_epoch = max(1, len(enantiomer_pairs_train) // (batch_size // 2))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx in range(num_batches_per_epoch):\n",
    "            batch_ws, batch_vs, batch_scores = create_enantiomer_batch(\n",
    "                enantiomer_pairs_train, dataG_train, labelsG_train, batch_size\n",
    "            )\n",
    "            \n",
    "            loss = generator.train_on_batch([batch_ws, batch_vs], batch_scores)\n",
    "            \n",
    "            if isinstance(loss, (list, tuple)):\n",
    "                loss = float(loss[0])\n",
    "            else:\n",
    "                loss = float(loss)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches_per_epoch\n",
    "        print(f\"Training loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        val_accuracy = evaluate_ranking_accuracy(generator, dataG_val, labelsG_val, val_df)\n",
    "        print(f\"Validation ranking accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_weights = generator.get_weights()\n",
    "            patience_counter = 0\n",
    "            print(f\"New best model saved with accuracy: {best_val_accuracy:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_weights is not None:\n",
    "        generator.set_weights(best_weights)\n",
    "    return generator, best_val_accuracy\n",
    "\n",
    "def evaluate_ranking_accuracy(model, data, labels, df):\n",
    "    \"\"\"\n",
    "    Evaluate ranking accuracy \n",
    "    \"\"\"\n",
    "    print(\"Getting predictions for all conformers...\")\n",
    "    \n",
    "    batch_size = 32\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(data[0]), batch_size):\n",
    "        batch_ws = data[0][i:i+batch_size]\n",
    "        batch_vs = data[1][i:i+batch_size]\n",
    "        batch_preds = model.predict([batch_ws, batch_vs], verbose=0)\n",
    "        all_predictions.extend(batch_preds.flatten())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    pairs = get_enantiomer_pairs(df)\n",
    "    \n",
    "    correct_rankings = 0\n",
    "    total_pairs = len(pairs)\n",
    "    \n",
    "    for pair in pairs:\n",
    "        preds1 = [all_predictions[idx] for idx in pair['enantiomer1_conformers']]\n",
    "        preds2 = [all_predictions[idx] for idx in pair['enantiomer2_conformers']]\n",
    "        \n",
    "        avg_pred1 = np.mean(preds1)\n",
    "        avg_pred2 = np.mean(preds2)\n",
    "        \n",
    "        true_score1 = pair['enantiomer1_score']\n",
    "        true_score2 = pair['enantiomer2_score']\n",
    "        \n",
    "        pred_ranking = avg_pred1 < avg_pred2\n",
    "        true_ranking = true_score1 < true_score2\n",
    "        \n",
    "        if pred_ranking == true_ranking:\n",
    "            correct_rankings += 1\n",
    "    \n",
    "    accuracy = correct_rankings / total_pairs if total_pairs > 0 else 0\n",
    "    print(f\"Evaluated {total_pairs} pairs, {correct_rankings} correct rankings\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "dataG_train = [np.array(dataG_train[0], dtype='float32'), np.array(dataG_train[1], dtype='float32')]\n",
    "labelsG_train = np.array(labelsG_train, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "dataG_val = [np.array(dataG_val[0], dtype='float32'), np.array(dataG_val[1], dtype='float32')]\n",
    "labelsG_val = np.array(labelsG_val, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "dataG_test = [np.array(dataG_test[0], dtype='float32'), np.array(dataG_test[1], dtype='float32')]\n",
    "labelsG_test = np.array(labelsG_test, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "print(\"Starting training with enantiomer pair sampling...\")\n",
    "print(f\"Training data: {len(train_df)} conformers\")\n",
    "print(f\"Validation data: {len(val_df)} conformers\") \n",
    "print(f\"Test data: {len(test_df)} conformers\")\n",
    "\n",
    "trained_generator, best_val_accuracy = train_with_enantiomer_sampling(\n",
    "    generator, dataG_train, labelsG_train, train_df,\n",
    "    dataG_val, labelsG_val, val_df,\n",
    "    epochs=10, batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_accuracy = evaluate_ranking_accuracy(trained_generator, dataG_test, labelsG_test, test_df)\n",
    "print(f\"Final test ranking accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nSaving the best model...\")\n",
    "model_path = \"best_docking_model.keras\" \n",
    "if os.path.exists(model_path):\n",
    "    os.remove(model_path)\n",
    "    print(f\"Removed existing model file: {model_path}\")\n",
    "\n",
    "trained_generator.save(model_path)\n",
    "print(\"Best model saved as 'best_docking_model.keras'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e76c4-83e9-4414-b1c0-3c0b273e2076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
